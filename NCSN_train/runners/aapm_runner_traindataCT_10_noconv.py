# aapm_runner_traindataCT_10_noconv.py
# è¯¥è„šæœ¬å®ç°åŸºäºæ‰©æ•£æ¨¡å‹çš„CTå›¾åƒé‡å»ºè®­ç»ƒä¸æµ‹è¯•æµç¨‹
# åŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ä¸ç»“æœä¿å­˜
# -------------------- ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ --------------------
import numpy as np
import tqdm
from ..losses.dsm import anneal_dsm_score_estimation
from ..losses.sliced_sm import anneal_sliced_score_estimation_vr
import torch.nn.functional as F
import logging
import torch
import matplotlib.pyplot as plt
import random
import pydicom as dicom
import wandb  # <<< æ–°å¢

# -------------------- Pythonæ ‡å‡†åº“å¯¼å…¥ --------------------
import os
import glob
import glob
import scipy.io as scio
import shutil

# import tensorboardX
import torch.optim as optim
from torchvision.datasets import CIFAR10
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler

# -------------------- å†…éƒ¨æ¨¡å—å¯¼å…¥ --------------------
from ..models.cond_refinenet_dilated_noconv import CondRefineNetDilated, CondRefineNetDeeperDilated
from torchvision.utils import save_image, make_grid
from PIL import Image
from .multiCTmain import FanBeam

__all__ = ['AapmRunnerdata_10C']
fanBeam = FanBeam()
randomData = [60, 120, 240]


# ç±» DatasetLoaderï¼šåŒæ—¶åŠ è½½è¾“å…¥å›¾åƒã€æ ‡ç­¾å›¾åƒå’ŒæŠ•å½±æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œè¿”å›å¯¹åº”çš„ä¸‰å…ƒç»„
class DatasetLoader(Dataset):
    # __init__ï¼šæ¥å—è¾“å…¥ã€æ ‡ç­¾ã€æŠ•å½±æ•°æ®æ ¹ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰ .npy æ–‡ä»¶è·¯å¾„å¹¶æ ¡éªŒæ•°é‡ä¸€è‡´æ€§
    def __init__(self, input_root, label_root, prj_root):
        self.input_files = sorted(glob.glob(os.path.join(input_root, '*.npy')))
        self.label_files = sorted(glob.glob(os.path.join(label_root, '*.npy')))
        self.prj_files   = sorted(glob.glob(os.path.join(prj_root,   '*.npy')))
        if not (len(self.input_files) == len(self.label_files) == len(self.prj_files)):
            raise ValueError("Input, label and proj file counts do not match")

    # __getitem__ï¼šæ ¹æ®ç´¢å¼•åŠ è½½å¯¹åº”çš„ .npy æ–‡ä»¶ï¼Œè¿”å› torch.Tensor æ ¼å¼çš„æ•°æ®
    def __getitem__(self, index):
        input_data = torch.from_numpy(np.load(self.input_files[index])).float()
        label_data = torch.from_numpy(np.load(self.label_files[index])).float()
        prj_data   = torch.from_numpy(np.load(self.prj_files[index])).float()
        return input_data, label_data, prj_data

    # __len__ï¼šè¿”å›æ•°æ®é›†æ ·æœ¬æ€»æ•°
    def __len__(self):
        return len(self.input_files)


# å‡½æ•° get_data_loadersï¼šæ ¹æ®ç»™å®šçš„è¾“å…¥/æ ‡ç­¾/æŠ•å½±æ ¹ç›®å½•ï¼Œåˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›† DataLoaderï¼Œæ”¯æŒæ¯”ä¾‹åˆ’åˆ†
def get_data_loaders(input_root, label_root, prj_root, batch_size, val_split=0.1):
    dataset = DatasetLoader(input_root, label_root, prj_root)
    size = len(dataset)
    indices = list(range(size))
    # æ ¹æ® val_split æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†ç´¢å¼•
    split = int(size * val_split)
    val_idx = indices[:split]
    train_idx = indices[split:]
    train_sampler = SubsetRandomSampler(train_idx)
    val_sampler   = SubsetRandomSampler(val_idx)
    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
    val_loader   = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
    return train_loader, val_loader


# ç±» TestsetLoaderï¼šåŠ è½½æµ‹è¯•é›†æ•°æ®ï¼Œè¿”å›è¾“å…¥ã€æ ‡ç­¾ã€æŠ•å½±å’Œæ–‡ä»¶å
class TestsetLoader(Dataset):
    def __init__(self, input_root, label_root, prj_root):
        self.input_files = sorted(glob.glob(os.path.join(input_root, '*.npy')))
        self.label_files = sorted(glob.glob(os.path.join(label_root, '*.npy')))
        self.prj_files   = sorted(glob.glob(os.path.join(prj_root,   '*.npy')))
        if not (len(self.input_files) == len(self.label_files) == len(self.prj_files)):
            raise ValueError("Input, label and proj file counts do not match")
    def __getitem__(self, index):
        input_data = torch.from_numpy(np.load(self.input_files[index])).float()
        label_data = torch.from_numpy(np.load(self.label_files[index])).float()
        prj_data   = torch.from_numpy(np.load(self.prj_files[index])).float()
        res_name   = os.path.basename(self.input_files[index])
        return input_data, label_data, prj_data, res_name
    def __len__(self):
        return len(self.input_files)


# å‡½æ•° get_test_loaderï¼šåˆ›å»ºæµ‹è¯•é›† DataLoaderï¼Œä¸æ‰“ä¹±é¡ºåº
def get_test_loader(input_root, label_root, prj_root, batch_size):
    dataset = TestsetLoader(input_root, label_root, prj_root)
    return DataLoader(dataset, batch_size=batch_size, shuffle=False)


# ç±» AapmRunnerdata_10Cï¼šå°è£…æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°é€»è¾‘
class AapmRunnerdata_10C():
    # __init__ï¼šåˆå§‹åŒ–å‚æ•°ã€é…ç½®åŠ wandb æ—¥å¿—
    def __init__(self, args, config):
        self.args = args
        self.config = config

        # å°è¯•åˆå§‹åŒ– Weights & Biasesï¼Œç”¨äºå®éªŒè¿½è¸ª
        try:
            wandb.init(
                project=getattr(args, "wandb_project", "aapm-ct-diffusion"),
                entity=getattr(args, "wandb_entity", None),
                config={**vars(args), **vars(config)} if hasattr(args, '__dict__') else None,
                name=args.doc if hasattr(args, 'doc') else None,
                resume="allow",
                id=getattr(args, "wandb_run_id", None)
            )
            print("[wandb] Initialized successfully.")
        except Exception as e:
            print(f"[wandb] Initialization failed: {e}")

    # get_optimizerï¼šæ ¹æ®é…ç½®é€‰æ‹©å¹¶è¿”å›æŒ‡å®šä¼˜åŒ–å™¨
    def get_optimizer(self, parameters):
        if self.config.optim.optimizer == 'Adam':
            return optim.Adam(parameters, lr=self.config.optim.lr, weight_decay=self.config.optim.weight_decay,
                              betas=(self.config.optim.beta1, 0.999), amsgrad=self.config.optim.amsgrad)
        elif self.config.optim.optimizer == 'RMSProp':
            return optim.RMSprop(parameters, lr=self.config.optim.lr, weight_decay=self.config.optim.weight_decay)
        elif self.config.optim.optimizer == 'SGD':
            return optim.SGD(parameters, lr=self.config.optim.lr, momentum=0.9)
        else:
            raise NotImplementedError('Optimizer {} not understood.'.format(self.config.optim.optimizer))

    # logit_transformï¼šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œ logit å˜æ¢ï¼Œä»¥åŒ¹é…æ¨¡å‹è¾“å…¥åˆ†å¸ƒ
    def logit_transform(self, image, lam=1e-6):
        image = lam + (1 - 2 * lam) * image
        return torch.log(image) - torch.log1p(-image)

    # trainï¼šè®­ç»ƒä¸»å‡½æ•°ï¼Œè´Ÿè´£æ•°æ®é¢„å¤„ç†ã€åŠ è½½ã€è®­ç»ƒå¾ªç¯ã€å®šæœŸè¯„ä¼°ä¸ä¿å­˜
    def train(self):
        # æ„å»ºè®­ç»ƒ/æµ‹è¯•æ•°æ®é¢„å¤„ç†ç®¡é“
        if self.config.data.random_flip is False:
            tran_transform = test_transform = transforms.Compose([
                transforms.Resize(self.config.data.image_size),
                transforms.ToTensor()
            ])
        else:
            tran_transform = transforms.Compose([
                transforms.Resize(self.config.data.image_size),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor()
            ])
            test_transform = transforms.Compose([
                transforms.Resize(self.config.data.image_size),
                transforms.ToTensor()
            ])

        # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©æ•°æ®åŠ è½½æ–¹å¼
        if self.config.data.dataset == 'CIFAR10':
            dataset = CIFAR10(os.path.join(self.args.run, 'datasets', 'cifar10'), train=True, download=True,
                              transform=tran_transform)
            test_dataset = CIFAR10(os.path.join(self.args.run, 'datasets', 'cifar10_test'), train=False, download=True,
                                   transform=test_transform)
        elif self.config.data.dataset == "AAPM":
            print("train")
        # è·å– AAPM æ•°æ®é›†çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯• DataLoader
        input_root = os.path.join(self.args.run, 'datasets', 'input_npy')  # è°ƒæ•´ä¸ºå®é™…è·¯å¾„
        label_root = os.path.join(self.args.run, 'datasets', 'label_npy')
        prj_root   = os.path.join(self.args.run, 'datasets', 'prj_npy')
        train_loader, val_loader = get_data_loaders(input_root, label_root, prj_root, self.config.training.batch_size, val_split=0.1)
        test_loader = get_test_loader(input_root.replace('train', 'test'),
                                      label_root.replace('train', 'test'),
                                      prj_root.replace('train', 'test'),
                                      self.config.training.batch_size)

        test_iter = iter(test_loader)
        self.config.input_dim = self.config.data.image_size ** 2 * self.config.data.channels
        tb_path = os.path.join(self.args.run, 'tensorboard', self.args.doc)
        if os.path.exists(tb_path):
            shutil.rmtree(tb_path)

        score = CondRefineNetDilated(self.config).to(self.config.device)
        score = torch.nn.DataParallel(score)
        optimizer = self.get_optimizer(score.parameters())

        # æ¢å¤è®­ç»ƒ
        if self.args.resume_training:
            states = torch.load(os.path.join(self.args.log, 'checkpoint_10000.pth'))
            score.load_state_dict(states[0])
            optimizer.load_state_dict(states[1])
            step = 43000
        else:
            step = 1

        sigmas = torch.tensor(
            np.exp(np.linspace(np.log(self.config.model.sigma_begin), np.log(self.config.model.sigma_end),
                               self.config.model.num_classes))).float().to(self.config.device)

        # è¿­ä»£è®­ç»ƒå¤šä¸ª epoch
        for epoch in range(self.config.training.n_epochs):
            # éå†ä¸€ä¸ª batchï¼Œæ‰§è¡Œå‰å‘ã€åå‘ä¼ æ’­å’Œä¼˜åŒ–
            for i, X in enumerate(train_loader):
                # print("ğŸŒ¶ï¸")
                print(X[0].shape)
                step += 1
                score.train()
                X = tuple(torch.from_numpy(x).float() if isinstance(x, np.ndarray) else x for x in X)
                X = tuple(x.to(self.config.device) for x in X)
                input_data = X[0]
                input_data = input_data / 256. * 255. + torch.rand_like(input_data) / 256.
                if self.config.data.logit_transform:
                    input_data = self.logit_transform(input_data)
                labels = torch.randint(0, len(sigmas), (input_data.shape[0],), device=input_data.device)
                if self.config.training.algo == 'dsm':
                    # print("ğŸ‘‹ğŸ‘‹ğŸ‘‹ dsm")
                    loss = anneal_dsm_score_estimation(score, input_data, labels, sigmas, self.config.training.anneal_power)
                elif self.config.training.algo == 'ssm':
                    # print("ğŸ‘‹ğŸ‘‹ğŸ‘‹ ssm")
                    loss = anneal_sliced_score_estimation_vr(score, input_data, labels, sigmas, n_particles=self.config.training.n_particles)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # wandb è®°å½• loss
                if wandb.run:
                    wandb.log({'epoch': epoch + 1, 'step': step, 'train_loss': loss.item()})
                logging.info("step: {}, loss: {}".format(step, loss.item()))

                if step >= self.config.training.n_iters:
                    if wandb.run:
                        wandb.finish()
                    return 0

                # æ¯éš”å›ºå®šæ­¥æ•°åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°
                if step % 100 == 0:
                    score.eval()
                    try:
                        test_X = next(test_iter)
                    except StopIteration:
                        test_iter = iter(test_loader)
                        test_X = next(test_iter)
                    test_X = tuple(torch.from_numpy(x).float() if isinstance(x, np.ndarray) else x for x in test_X)
                    test_X = tuple(x.to(self.config.device) for x in test_X)
                    test_input = test_X[0]
                    test_input = test_input / 256. * 255. + torch.rand_like(test_input) / 256.
                    if self.config.data.logit_transform:
                        test_input = self.logit_transform(test_input)
                    test_labels = torch.randint(0, len(sigmas), (test_input.shape[0],), device=test_input.device)
                    with torch.no_grad():
                        # print('ğŸ’ğŸ’ğŸ’')
                        test_dsm_loss = anneal_dsm_score_estimation(score, test_input, test_labels, sigmas, self.config.training.anneal_power)
                        # print(f'ğŸº score: {score}')
                    if wandb.run:
                        wandb.log({'step': step, 'test_loss': test_dsm_loss.item()})

                # å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
                if step % self.config.training.snapshot_freq == 0:
                    states = [
                        score.state_dict(),
                        optimizer.state_dict(),
                    ]
                    torch.save(states, os.path.join(self.args.log, 'checkpoint_{}.pth'.format(step)))
                    torch.save(states, os.path.join(self.args.log, 'checkpoint.pth'))
                    # wandb artifactï¼ˆå¯é€‰ï¼‰
                    if wandb.run:
                        try:
                            artifact = wandb.Artifact(f'checkpoint_{step}', type='model')
                            artifact.add_file(os.path.join(self.args.log, f'checkpoint_{step}.pth'))
                            wandb.log_artifact(artifact)
                        except Exception as e:
                            print(f"[wandb] artifact failed: {e}")

        if wandb.run:
            wandb.finish()
        print("Training completed.")
